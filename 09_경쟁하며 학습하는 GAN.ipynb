{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d54b3d11",
   "metadata": {},
   "source": [
    "# í•™ìŠµ ëª©í‘œ\n",
    "GANì„ ì´ìš©í•´ ì›í•˜ëŠ” ì´ë¯¸ì§€ë¥¼ ìƒì„±í•œë‹¤.\n",
    "\n",
    "\n",
    "\n",
    "# ëª©ì°¨\n",
    "\n",
    "- GANì˜ ê¸°ì´ˆ\n",
    "- GANìœ¼ë¡œ ìƒˆë¡œìš´ íŒ¨ì…˜ ì•„ì´í…œ ìƒì„±í•˜ê¸°\n",
    "- cGANìœ¼ë¡œ ìƒì„± ì œì–´í•˜ê¸°\n",
    "\n",
    "\n",
    "\n",
    "### 9.1 GAN ê¸°ì´ˆ\n",
    "\n",
    "**GAN(=Generative Adversarial Network)** ì§ì—­í•˜ë©´ ì ëŒ€ì  ìƒì„± ì‹ ê²½ë§ì´ë‹¤. ë‹¨ì–´ë¥¼ í•˜ë‚˜ì”© ëœ¯ì–´ì„œ ì´í•´í•˜ë©´ í¸í•˜ë‹¤.\n",
    "1. **Generative** : GANì€ ìƒì„±ì„ í•˜ëŠ” ëª¨ë¸ì´ë‹¤. CNN, RNNê³¼ëŠ” ë‹¬ë¦¬ GNNì€ ìƒˆë¡œìš´ ì´ë¯¸ì§€ë‚˜ ìŒì„±ì„ **ì°½ì‘**í•˜ë„ë¡ ê³ ì•ˆë˜ì—ˆë‹¤.\n",
    "2. **Adversarial** : GANì€ ì ëŒ€ì ìœ¼ë¡œ í•™ìŠµí•œë‹¤. ê°€ì§œ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ëŠ” **ìƒì„±ì**ì™€ ì´ë¯¸ì§€ì˜ ì§„ìœ„ë¥¼ íŒë³„í•˜ëŠ” **íŒë³„ì**ê°€ ë²ˆê°ˆì•„ í•™ìŠµí•˜ë©° ê²½ìŸì ìœ¼ë¡œ í•™ìŠµì„ ì§„í–‰í•œë‹¤.\n",
    "3. **Network** : GANì€ ì¸ê³µ ì‹ ê²½ë§ ëª¨ë¸ì´ë‹¤. ìƒì„±ì, íŒë³„ìê°€ ëª¨ë‘ ì‹ ê²½ë§ìœ¼ë¡œ ë˜ì–´ìˆë‹¤.\n",
    "\n",
    "GANì€ ë¹„ì§€ë„í•™ìŠµ ë°©ì‹ì´ë‹¤. ë¹„ì§€ë„í•™ìŠµì´ ë¯¸ë˜ì§€í–¥ì ì´ë¼ëŠ” í‰ê°€ë¥¼ ë°›ëŠ” ì´ìœ ëŠ” ëŒ€ë¶€ë¶„ì˜ ë°ì´í„°ì—ëŠ” ì •ë‹µ(label)ì´ ì—†ê¸° ë•Œë¬¸ì´ë‹¤. ë¹„ì§€ë„í•™ìŠµì€ ì‚¬ëŒì˜ ì†ê¸¸ì„ ìµœì†Œí™”í•˜ë©° í•™ìŠµí•  ìˆ˜ ìˆë‹¤.\n",
    "\n",
    "\n",
    "#### 9.1.1 ìƒì„±ìì™€ íŒë³„ì\n",
    "- **ìƒì„±ì** : ë¬´ì‘ìœ„ í…ì„œë¡œë¶€í„° ì—¬ëŸ¬ ê°€ì§€ í˜•íƒœì˜ ê°€ì§œ ì´ë¯¸ì§€ë¥¼ ìƒì„±.\n",
    "- **íŒë³„ì** : ì§„ì§œ ì´ë¯¸ì§€ì™€ ê°€ì§œ ì´ë¯¸ì§€ë¥¼ êµ¬ë¶„.\n",
    "\n",
    "í•™ìŠµì´ ì§„í–‰ë˜ë©° ìƒì„±ìëŠ” íŒë³„ìë¥¼ ì†ì´ê¸° ìœ„í•´ ì ì  ì •ë°€í•œ ê°€ì§œ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ê³ , íŒë³„ìëŠ” í•™ìŠµ ë°ì´í„°ì—ì„œ ê°€ì ¸ì˜¨ ì§„ì§œ ì´ë¯¸ì§€ì™€ ìƒì„±ìê°€ ë§Œë“  ê°€ì§œ ì´ë¯¸ì§€ë¥¼ ì ì  ë” ì˜ êµ¬ë³„í•˜ê²Œ ëœë‹¤.\n",
    "\n",
    "> ğŸ’¡ ì°¸ê³  ë§í¬ : [GANì˜ ê¸°ì´ˆ](https://velog.io/@tobigs-gm1/basicofgan)\n",
    "\n",
    "---\n",
    "\n",
    "### 9.2. GANìœ¼ë¡œ ìƒˆë¡œìš´ íŒ¨ì…˜ ì•„ì´í…œ ìƒì„±í•˜ê¸°\n",
    "#### 9.2.1 í•™ìŠµ ì¤€ë¹„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9edd23de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ import\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision.utils import save_image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f36d1a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë‹¤ìŒ ì¥ì¹˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤:  cpu\n"
     ]
    }
   ],
   "source": [
    "# í•˜ì´í¼íŒŒë¼ë¯¸í„°\n",
    "EPOCHS = 500\n",
    "BATCH_SIZE = 100\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
    "print(\"ë‹¤ìŒ ì¥ì¹˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤: \", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e54f5f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•™ìŠµ ë°ì´í„°ì…‹ ê°€ì ¸ì˜¤ê¸°\n",
    "# Fashion MNIST ë°ì´í„°ì…‹\n",
    "trainset = datasets.FashionMNIST('./.data',\n",
    "                                train = True,\n",
    "                                download = True,\n",
    "                                transform = transforms.Compose([\n",
    "                                    transforms.ToTensor(), # í…ì„œë¡œ ë°”ê¿”ì£¼ê¸°\n",
    "                                    transforms.Normalize((0.5,), (0.5,)) # ì •ê·œí™”\n",
    "                                ]))\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset = trainset,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3eefa97",
   "metadata": {},
   "source": [
    "#### 9.2.2 ìƒì„±ìì™€ íŒë³„ì êµ¬í˜„\n",
    "ì´ì œê¹Œì§€ëŠ” `nn.Module`í´ë˜ìŠ¤ë¥¼ ìƒì†ë°›ëŠ” í´ë˜ìŠ¤ë¡œ ì •ì˜í–ˆê¸° ë•Œë¬¸ì— ëª¨ë¸ì˜ ë³µì¡í•œ ë™ì‘ë“¤ì„ í•¨ìˆ˜ë¡œ ì •ì˜ ê°€ëŠ¥í–ˆë‹¤. í•˜ì§€ë§Œ ì´ë²ˆ ìƒì„±ì, íŒë³„ìëŠ” ê°€ë…ì„±ì„ ìœ„í•´ ìµœëŒ€í•œ ê°„ë‹¨í•˜ê²Œ ë§Œë“¤ê³ ì í•œë‹¤.\n",
    "\n",
    "`Sequential` í´ë˜ìŠ¤ë¡œ ì‹ ê²½ë§ì„ ì´ë£¨ëŠ” ê° ì¸µì—ì„œ ìˆ˜í–‰í•  ì—°ì‚°ë“¤ì„ ì…ë ¥ë°›ì•„ ì°¨ë¡€ë¡œ ì‹¤í–‰í•˜ê³ ì í•œë‹¤. -> `__init__()`ê³¼ `forward()` í•¨ìˆ˜ë¥¼ ë™ì‹œì— ì •ì˜."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a81868b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìƒì„±ì(Generator)\n",
    "G = nn.Sequential(\n",
    "    nn.Linear(64, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, 784), # ê²°ê³¼ê°’ì´ Fashion MINST ì´ë¯¸ì§€ì™€ ê°™ì€ ì°¨ì›ì˜ í…ì„œê°€ ë˜ì–´ì•¼í•¨.\n",
    "    nn.Tanh() # íƒ„ì  íŠ¸ í•¨ìˆ˜ ìƒê°í•˜ë©´ë¨. ê°’ì„ -1~1 ì‚¬ì´ë¡œ ì••ì¶•í•´ì¤Œ.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d8408d",
   "metadata": {},
   "source": [
    "íŒë³„ìì—ì„œëŠ” ReLUê°€ ì•„ë‹ˆë¼ **Leaky ReLU** í™œì„±í™” í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•œë‹¤.\n",
    "\n",
    "Leaky ReLUí™œì„±í™” í•¨ìˆ˜ëŠ” ì•½ê°„ì˜ ìŒì˜ ê¸°ìš¸ê¸°ë„ ë‹¤ìŒ ì¸µì— ì „ë‹¬í•˜ëŠ” ì—­í• ì„ í•˜ëŠ”ë°, ì´ë ‡ê²Œ í•˜ë©´ íŒë³„ìì—ì„œ ê³„ì‚°ëœ ê¸°ìš¸ê¸°ê°€ 0 ëŒ€ì‹  ì•½í•œ ìŒìˆ˜ë¡œ ì „í™˜ë˜ë©° **ìƒì„±ìì— ë” ê°•í•˜ê²Œ ì „ë‹¬**ë˜ê¸° ë•Œë¬¸ì´ë‹¤.\n",
    "\n",
    "ìƒì„±ìê°€ í•™ìŠµí•˜ê¸° ìœ„í•´ íŒë³„ìë¡œë¶€í„° **ê¸°ìš¸ê¸°ë¡¤ íš¨ê³¼ì ìœ¼ë¡œ ì „ë‹¬**ë°›ì•„ì•¼ í•˜ê¸° ë•Œë¬¸ì— ì¤‘ìš”í•˜ë‹¤!\n",
    "\n",
    "![ReLUì™€ LeakyReLU ì°¨ì´](https://github.com/kjsoo-1010/pytorch_deeplearning/blob/main/9_ReLU_LeakyReLU.png?raw=true)\n",
    "[ì´ë¯¸ì§€ ì¶œì²˜](https://atcold.github.io/pytorch-Deep-Learning/ko/week11/11-1/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39c78e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# íŒë³„ì(Discriminator)\n",
    "D = nn.Sequential(\n",
    "    nn.Linear(784, 256),\n",
    "    nn.LeakyReLU(0.2),\n",
    "    nn.Linear(256, 256),\n",
    "    nn.LeakyReLU(0.2),\n",
    "    nn.Linear(256, 1), # ì§„ì§œ/ê°€ì§œ ë¡œ ê²°ì •ì´ ë˜ê¸° ë–„ë¬¸ì— ìµœì¢… ì¶œë ¥ê°’ = 1\n",
    "    nn.Sigmoid()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dba561f",
   "metadata": {},
   "source": [
    "#### 9.2.3 GAN í•™ìŠµ êµ¬í˜„\n",
    "ìƒì„±ìì™€ íŒë³„ì í•™ìŠµì— ì“°ì¼ ì˜¤ì°¨ í•¨ìˆ˜ì™€ ìµœì í™” ì•Œê³ ë¦¬ì¦˜ì„ ê°ê° ì •ì˜í•´ì¤€ë‹¤.\n",
    "\n",
    "- ì˜¤ì°¨ í•¨ìˆ˜ : ë ˆì´ë¸”ì´ ê°€ì§œ/ì§„ì§œ 2ê°€ì§€ ë¿ì´ë¯€ë¡œ BCE(ì´ì§„ êµì°¨ ì—”íŠ¸ë¡œí”¼)ë¥¼ ì‚¬ìš©.\n",
    "- ìµœì í™” í•¨ìˆ˜ : Adam (ì œì¼ ë¬´ë‚œí•˜ê³  ë¹ ë¥´ë‹¤!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41ede954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë¸ ê°€ì¤‘ì¹˜ ì§€ì • ì¥ì¹˜ì— ë³´ë‚´ê¸°\n",
    "D = D.to(DEVICE)\n",
    "G = G.to(DEVICE)\n",
    "\n",
    "# ì˜¤ì°¨ í•¨ìˆ˜ : ì§„ì§œ/ê°€ì§œ ë‘ ì¢…ë¥˜ë‹ˆê¹Œ BCE\n",
    "# ìµœì í™” í•¨ìˆ˜ : Adam\n",
    "criterion = nn.BCELoss()\n",
    "d_optimizer = optim.Adam(D.parameters(), lr = 0.002)\n",
    "g_optimizer = optim.Adam(G.parameters(), lr = 0.002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3abe746",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/500], d_loss: 0.0000, g_loss: 100.0000, D(x): 1.00, D(G(z)): 0.00\n",
      "Epoch [1/500], d_loss: 0.0000, g_loss: 100.0000, D(x): 1.00, D(G(z)): 0.00\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [7], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m total_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCHS):\n\u001b[1;32m----> 3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, (images, _) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[0;32m      4\u001b[0m         images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mreshape(BATCH_SIZE, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[0;32m      6\u001b[0m         \u001b[38;5;66;03m# 'ì§„ì§œ'ì™€ 'ê°€ì§œ' ë ˆì´ë¸” ìƒì„±\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    679\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    680\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 681\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    682\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    683\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    684\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    685\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    719\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    720\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 721\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    722\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    723\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\datasets\\mnist.py:145\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    142\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img\u001b[38;5;241m.\u001b[39mnumpy(), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 145\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    148\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\transforms\\transforms.py:94\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 94\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\transforms\\transforms.py:134\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\transforms\\functional.py:168\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pic\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    167\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m255\u001b[39m \u001b[38;5;241m*\u001b[39m img\n\u001b[1;32m--> 168\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetbands\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;66;03m# put it from HWC to CHW format\u001b[39;00m\n\u001b[0;32m    170\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mpermute((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mcontiguous()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# í•™ìŠµì‹œí‚¤ëŠ” ë°˜ë³µë¬¸ ì‹œì‘\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(EPOCHS):\n",
    "    for i, (images, _) in enumerate(train_loader):\n",
    "        images = images.reshape(BATCH_SIZE, -1).to(DEVICE) # batch ì‚¬ì´ì¦ˆë¡œ í…ì„œ í¬ê¸° ë°”ê¿”ì£¼ê¸°\n",
    "        \n",
    "        ### 1. 'ì§„ì§œ'ì™€ 'ê°€ì§œ' ë ˆì´ë¸” ìƒì„±\n",
    "        # ìƒì„±ìê°€ ë§Œë“  ë°ì´í„° = zeros() ë¡œ 0ìœ¼ë¡œ ì±„ì›Œì„œ ë¼ë²¨ë§ í•´ì£¼ê¸°\n",
    "        # Fashion MINST ë°ì´í„° = ones() ë¡œ 1ë¡œ ì±„ì›Œì„œ ë¼ë²¨ë§ í•´ì£¼ê¸°\n",
    "        real_labels = torch.ones(BATCH_SIZE, 1).to(DEVICE)\n",
    "        fake_labels = torch.zeros(BATCH_SIZE, 1).to(DEVICE)\n",
    "        \n",
    "        ### 2. íŒë³„ìê°€ ì§„ì§œ ì´ë¯¸ì§€ë¥¼ ì§„ì§œë¡œ ì¸ì‹í•˜ëŠ” ì˜¤ì°¨ë¥¼ ì˜ˆì‚°\n",
    "        outputs = D(images) # ì‹¤ì œ ì´ë¯¸ì§€ -> íŒë³„ì ì‹ ê²½ë§ ê²°ê³¼ê°’\n",
    "        d_loss_real = criterion(outputs, real_labels) # ì§„ì§œ ë ˆì´ë¸”ê°„ì˜ ì˜¤ì°¨ ê³„ì‚°\n",
    "        real_score = outputs\n",
    "        \n",
    "        ### 3. ìƒì„±ì ë™ì‘ ì •ì˜\n",
    "        # ìƒì„±ìëŠ” ì •ê·œë¶„í¬ë¡œë¶€í„° ìƒì„±í•œ ë¬´ì‘ìœ„ í…ì„œ -> ì‹¤ì œ ì´ë¯¸ì§€ì™€ ì°¨ì› ê°™ì€ í…ì„œ ë°°ì¶œ\n",
    "        z = torch.randn(BATCH_SIZE, 64).to(DEVICE)\n",
    "        fake_images = G(z)\n",
    "        \n",
    "        ### 4. íŒë³„ìê°€ ê°€ì§œ ì´ë¯¸ì§€ë¥¼ ê°€ì§œë¡œ ì¸ì‹í•˜ëŠ” ì˜¤ì°¨ë¥¼ ê³„ì‚°\n",
    "        # ìƒì„±ì ì´ë¯¸ì§€(fake_images) -> íŒë³„ìì— ì…ë ¥ -> ì˜¤ì°¨ ê³„ì‚°\n",
    "        outputs = D(fake_images)\n",
    "        d_loss_fake = criterion(outputs, fake_labels)\n",
    "        fake_score = outputs\n",
    "        \n",
    "        ### 5. ì§„ì§œì™€ ê°€ì§œ ì´ë¯¸ì§€ë¥¼ ê°–ê³  ë‚¸ ì˜¤ì°¨ë¥¼ ë”í•´ì„œ íŒë³„ìì˜ ì˜¤ì°¨ ê³„ì‚°\n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "\n",
    "        # ì—­ì „íŒŒë¡œ íŒë³„ì ëª¨ë¸ í•™ìŠµ ì§„í–‰ -> íŒë³„ì í•™ìŠµ!\n",
    "        d_optimizer.zero_grad()\n",
    "        g_optimizer.zero_grad()\n",
    "        d_loss.backward()\n",
    "        d_optimizer.step()\n",
    "        \n",
    "        ### 6. ìƒì„±ì í•™ìŠµ ì°¨ë¡€\n",
    "        # ìƒì„±ìê°€ íŒë³„ìë¥¼ ì†ì˜€ëŠ”ì§€ì— ëŒ€í•œ ì˜¤ì°¨ë¥¼ ê³„ì‚°\n",
    "        fake_images = G(z)\n",
    "        outputs = D(fake_images) # ìƒì„±ìì˜ ê²°ê³¼ë¬¼ì„ ë‹¤ì‹œ íŒë³„ìì— ì…ë ¥ì‹œì¼œ\n",
    "        g_loss = criterion(outputs, real_labels) # ê²°ê³¼ë¬¼ê³¼ 1 ì‚¬ì´ ì˜¤ì°¨ ìµœì†Œí™”í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ í•™ìŠµ ì§„í–‰\n",
    "        \n",
    "        ### 7. ì—­ì „íŒŒë¡œ ìƒì„±ì í•™ìŠµ ì§„í–‰\n",
    "        d_optimizer.zero_grad()\n",
    "        g_optimizer.zero_grad()\n",
    "        g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "        \n",
    "    ### 8. í•™ìŠµ ì§„í–‰ ì•Œì•„ë³´ê¸°\n",
    "    # d_loss = íŒë³„ì ì˜¤ì°¨, g_loss = ìƒì„±ì ì˜¤ì°¨\n",
    "    # D(x) = ì§„ì§œë¥¼ ì§„ì§œë¡œ ì¸ì‹í•œ 'ì •í™•ë„'\n",
    "    # D(G(z)) = ê°€ì§œë¥¼ ì§„ì§œë¡œ ì¸ì‹í•œ ì •í™•ë„\n",
    "    print('Epoch [{}/{}], d_loss: {:.4f}, g_loss: {:.4f}, D(x): {:.2f}, D(G(z)): {:.2f}' \n",
    "          .format(epoch, EPOCHS, d_loss.item(), g_loss.item(), \n",
    "                  real_score.mean().item(), fake_score.mean().item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c04aebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•™ìŠµ ëë‚œ ìƒì„±ì ê²°ê³¼ë¬¼ í™•ì¸\n",
    "z = torch.randn(BATCH_SIZE, 64).to(DEVICE)\n",
    "fake_images = G(z)\n",
    "for i in range(10):\n",
    "    fake_images_img = np.reshape(fake_images.data.cpu().numpy()[i],(28, 28))\n",
    "    plt.imshow(fake_images_img, cmap = 'gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6450ada2",
   "metadata": {},
   "source": [
    "#### 9.2.4 ê²°ê³¼ë¬¼ ì‹œê°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf824c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.randn(BATCH_SIZE, 64).to(DEVICE)\n",
    "fake_images = G(z)\n",
    "\n",
    "for i in range(10):\n",
    "    fake_images_img = np.reshape(fake_images.data.cpu().numpy()[i], (28, 28))\n",
    "    plt.imshow(fake_images_img, cmap = 'gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1487df2c",
   "metadata": {},
   "source": [
    "![GAN ê²°ê³¼ ì´ë¯¸ì§€](https://github.com/kjsoo-1010/pytorch_deeplearning/blob/main/9_GAN_image.png?raw=true)\n",
    "\n",
    "---\n",
    "\n",
    "### 9.3 cGANìœ¼ë¡œ ìƒì„± ì œì–´í•˜ê¸°\n",
    "GANê°€ ë”  ì“¸ëª¨ ìˆìœ¼ë ¤ë©´ ë¬´ì‘ìœ„ ìƒì„±ë³´ë‹¤ëŠ” **ì‚¬ìš©ìê°€ ì›í•˜ëŠ” ì´ë¯¸ì§€**ë¥¼ ìƒì„±í•˜ëŠ” ê¸°ëŠ¥ì„ ì œê³µí•´ì•¼í•œë‹¤.\n",
    "\n",
    "#### 9.3.1 cGANìœ¼ë¡œ ì›í•˜ëŠ” ì´ë¯¸ì§€ ìƒì„±í•˜ê¸°\n",
    "ì•ì—ì„œ í•œ GAN ëª¨ë¸ì€ 'íŒ¨ì…˜ ì•„ì´í…œ ì¢…ë¥˜ ì¤‘ ë¬´ì—‡ì„ ìƒì„±í•˜ë¼'ë¼ëŠ” ë¡œì§ì´ ì—†ì—ˆë‹¤.\n",
    "\n",
    "`ë¬´ì‘ìœ„ ë²¡í„° ì…ë ¥ -> ë¬´ì‘ìœ„ íŒ¨ì…˜ ì•„ì´í…œ ì¶œë ¥`\n",
    "\n",
    "ì¦‰, ì‚¬ìš©ìê°€ ì›í•˜ëŠ” íŒ¨ì…˜ ì•„ì´í…œì„ ìƒì„±í•˜ëŠ” ëŠ¥ë ¥ì´ ì—†ë‹¤.\n",
    "ì´ë¥¼ ë³´ì™„í•´ ì¶œë ¥í•  ì•„ì´í…œì˜ ì¢…ë¥˜ë¥¼ ì‚¬ìš©ìë¡œë¶€í„° ì…ë ¥ë°›ì•„ ê·¸ì— í•´ë‹¹í•˜ëŠ” ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ëŠ” ëª¨ë¸ì´ **ì¡°ê±´ë¶€ GAN**(=cGAN).\n",
    "\n",
    "\n",
    "![cGAN ëª¨ë¸ êµ¬ì¡°](https://github.com/kjsoo-1010/pytorch_deeplearning/blob/main/9_cGAN_model.png?raw=true)\n",
    "\n",
    "í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸, í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •, ë°ì´í„° ë¡œë”© ë¶€ë¶„ì€ GANê³¼ ê°™ë‹¤.\n",
    "\n",
    "#### 9.3.2 ì¡°ê±´ë¶€ ìƒì„±ìì™€ íŒë³„ì"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67c7ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìƒì„±ì (Generator)\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embed = nn.Embedding(10, 10)\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(110, 256), # ì™œ 110ê°œ? -> 100 + 10(ë ˆì´ë¸” ì •ë³´)\n",
    "            # nn.LeakyReLU(negative_slope=0.01, inplace=False)\n",
    "            # inplace = True -> ì…ë ¥ì„ ë³µì‚¬í•˜ì§€ ì•Šê³  ë°”ë¡œ ì¡°ì‘í•œë‹¤ëŠ” ëœ»\n",
    "            nn.LeakyReLU(0.2, inplace=True), \n",
    "            \n",
    "            nn.Linear(256, 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(1024, 784),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, z, labels): # ì…ë ¥ê³¼ ë ˆì´ë¸” ì •ë³´ê¹Œì§€ 2ê°€ì§€ ì…ë ¥ ë°›ìŒ.\n",
    "        # ë°°ì¹˜x1 í¬ê¸°ì˜ ë°°ì¹˜x10ì˜ ì—°ì†ì ì¸ í…ì„œë¡œ ì „í™˜\n",
    "        c = self.embed(labels)\n",
    "        \n",
    "        # cat() : ë‘ ë²¡í„°ë¥¼ ë‘ ë²ˆì§¸ ì¸ìˆ˜ ì°¨ì›ì— ëŒ€í•´ ì´ì–´ë¶™ì´ëŠ” ì—°ì‚° ì‹¤í–‰.\n",
    "        x = torch.cat([z, c], 1)\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f7daac",
   "metadata": {},
   "source": [
    "> ğŸ’¡ ì°¸ê³  ë§í¬ : [nn.Embedding()](https://wikidocs.net/64779)\n",
    "\n",
    "ë ˆì´ë¸”ì´ ì£¼ì–´ì¡Œì„ë•Œ ê°€ì§œì¸ í™•ë¥ ê³¼ ì§„ì§œì¸ í™•ë¥ ì„ ì¶”ì •í•œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d92e94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# íŒë³„ì (Discriminator)\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embed = nn.Embedding(10, 10)\n",
    "        \n",
    "        # ì„±ëŠ¥ ëŠ˜ë¦¬ê¸° ìœ„í•´ ë“œë¡­ì•„ì›ƒ ê³„ì¸µ 2ê°œ ë” ì¶”ê°€.\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(794, 1024), # 784 + 10(ë ˆì´ë¸” ì •ë³´ ì „ë‹¬)\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, labels):\n",
    "        c = self.embed(labels)\n",
    "        x = torch.cat([x, c], 1)\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef1264e",
   "metadata": {},
   "source": [
    "#### 9.3.3 cGAN í•™ìŠµ êµ¬í˜„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499c655f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë§Œë“¤ê³  ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ë¥¼ ì§€ì •í•œ ì¥ì¹˜ë¡œ ë³´ë‚´ê¸°\n",
    "D = Discriminator().to(DEVICE)\n",
    "G = Generator().to(DEVICE)\n",
    "\n",
    "# ì˜¤ì°¨ í•¨ìˆ˜,ìµœì í™” í•¨ìˆ˜ ì•ê³¼ ë™ì¼\n",
    "criterion = nn.BCELoss()\n",
    "d_optimizer = optim.Adam(D.parameters(), lr =0.0002)\n",
    "g_optimizer = optim.Adam(G.parameters(), lr =0.0002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6d67ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_step = len(train_loader)\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "     # ê·¸ëƒ¥ GANì—ì„  ë¼ë²¨ì´ í•„ìš” ì—†ì–´ (images, _) ì˜€ëŠ”ë°, ì´ë²ˆì—” ë°›ì•„ì„œ ì‚¬ìš©.\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.reshape(BATCH_SIZE, -1).to(DEVICE)\n",
    "        \n",
    "        # ì§„ì§œ/ê°€ì§œ ë ˆì´ë¸” ìƒì„±\n",
    "        real_labels = torch.ones(BATCH_SIZE, 1).to(DEVICE)\n",
    "        fake_labels = torch.zeros(BATCH_SIZE, 1).to(DEVICE)\n",
    "\n",
    "        # íŒë³„ìê°€ ì§„ì§œ ì´ë¯¸ì§€ë¥¼ ì§„ì§œë¡œ ì¸ì‹í•˜ëŠ” ì˜¤ì°¨ ê³„ì‚° (ë°ì´í„°ì…‹ ë ˆì´ë¸” ì…ë ¥)\n",
    "        labels = labels.to(DEVICE)\n",
    "        outputs = D(images, labels)\n",
    "        d_loss_real = criterion(outputs, real_labels)\n",
    "        real_score = outputs\n",
    "        \n",
    "        # ë¬´ì‘ìœ„ í…ì„œ ìƒì„± = g_label\n",
    "        z = torch.randn(BATCH_SIZE, 100).to(DEVICE)\n",
    "        g_label = torch.randint(0, 10, (BATCH_SIZE,)).to(DEVICE)\n",
    "        fake_images = G(z, g_label) # ë¬´ì‘ìœ„ í…ì„œì™€ ì…ë ¥ z ë¡œ ê°€ì§œ ì´ë¯¸ì§€ ìƒì„±\n",
    "        \n",
    "        # íŒë³„ìì—ê²Œ ê°€ì§œ ì´ë¯¸ì§€ ì…ë ¥.\n",
    "        # outputs = (ê°€ì§œ ì´ë¯¸ì§€, ê·¸ì— ëŒ€í•œ ë ˆì´ë¸”ê°’)ë¥¼ íŒë³„í•œ ê°’\n",
    "        outputs = D(fake_images, g_label)\n",
    "        d_loss_fake = criterion(outputs, fake_labels) # ì˜¤ì°¨ ê³„ì‚°\n",
    "        fake_score = outputs\n",
    "        \n",
    "        # loss_real = ì§„ì§œ ì´ë¯¸ì§€ ë„£ì—ˆì„ ë•Œ ì˜¤ì°¨\n",
    "        # loss_fake = ê°€ì§œ ì´ë¯¸ì§€ ë„£ì—ˆì„ ë•Œ ì˜¤ì°¨\n",
    "        d_loss = d_loss_real + d_loss_fake # ì´ ì˜¤ì°¨\n",
    "        \n",
    "        # ì—­ì „íŒŒë¡œ íŒë³„ì í•™ìŠµ ì§„í–‰\n",
    "        d_optimizer.zero_grad()\n",
    "        g_optimizer.zero_grad()\n",
    "        d_loss.backward()\n",
    "        d_optimizer.step()\n",
    "        \n",
    "        # ì•„ê¹Œ ìƒì„±í•´ë‘” zì™€ g_labelë¡œ ë‹¤ì‹œ ì´ë¯¸ì§€ ìƒì„± => íŒë³„ì ì†ì˜€ë‚˜?\n",
    "        # ìƒì„±ì, íŒë³„ì ë‘˜ ë‹¤ g_labelì„ ë¼ë²¨ë¡œ ë°›ê¸°\n",
    "        fake_images = G(z, g_label)\n",
    "        outputs = D(fake_images, g_label)\n",
    "        g_loss = criterion(outputs, real_labels)\n",
    "\n",
    "        # ëª» ì†ì¸ ë§Œí¼ ìƒì„±ì í•™ìŠµ ì§„í–‰\n",
    "        d_optimizer.zero_grad()\n",
    "        g_optimizer.zero_grad()\n",
    "        g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "        \n",
    "    print('ì´í­ [{}/{}] d_loss:{:.4f} g_loss: {:.4f} D(x):{:.2f} D(G(z)):{:.2f}'\n",
    "          .format(epoch,\n",
    "                  EPOCHS,\n",
    "                  d_loss.item(),\n",
    "                  g_loss.item(),\n",
    "                  real_score.mean().item(),\n",
    "                  fake_score.mean().item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f623898",
   "metadata": {},
   "source": [
    "ì´í­ [0/300] d_loss:0.3160 g_loss: 6.8871 D(x):0.89 D(G(z)):0.04\n",
    "ì´í­ [1/300] d_loss:0.5910 g_loss: 3.9762 D(x):0.87 D(G(z)):0.19\n",
    "ì´í­ [2/300] d_loss:0.3091 g_loss: 4.8783 D(x):0.92 D(G(z)):0.08\n",
    "ì´í­ [3/300] d_loss:0.1595 g_loss: 4.3155 D(x):0.95 D(G(z)):0.06\n",
    "ì´í­ [4/300] d_loss:0.5849 g_loss: 3.0070 D(x):0.78 D(G(z)):0.13\n",
    "...\n",
    "ì´í­ [295/300] d_loss:1.2423 g_loss: 1.0166 D(x):0.63 D(G(z)):0.43\n",
    "ì´í­ [296/300] d_loss:1.3765 g_loss: 0.9225 D(x):0.49 D(G(z)):0.44\n",
    "ì´í­ [297/300] d_loss:1.2616 g_loss: 0.8626 D(x):0.56 D(G(z)):0.45\n",
    "ì´í­ [298/300] d_loss:1.2170 g_loss: 0.8461 D(x):0.57 D(G(z)):0.43\n",
    "ì´í­ [299/300] d_loss:1.1471 g_loss: 1.2162 D(x):0.60 D(G(z)):0.38\n",
    "\n",
    "\n",
    "#### 9.3.4 ê²°ê³¼ë¬¼ ì‹œê°í™”\n",
    "\n",
    "`torch.full(í…ì„œ í¬ê¸°, í…ì„œ ì›ì†Œ ì´ˆê¸°í™”í•  ê°’)` : ìƒˆë¡œìš´ í…ì„œë¥¼ ë§Œë“œëŠ” í•¨ìˆ˜."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c52589b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë§Œë“¤ê³  ì‹¶ì€ ì•„ì´í…œ ìƒì„±í•˜ê³  ì‹œê°í™”í•˜ê¸°\n",
    "item_number = 9 # ì•„ì´í…œ ë²ˆí˜¸ : ë¶€ì¸ \n",
    "z = torch.randn(1, 100).to(DEVICE) # ë°°ì¹˜ í¬ê¸° 1\n",
    "\n",
    "# g_label = ì§€ì •í•œ ì•„ì´í…œ ë²ˆí˜¸\n",
    "g_label = torch.full((1,), item_number, dtype=torch.long).to(DEVICE)\n",
    "sample_images = G(z, g_label) # ì´ë¯¸ì§€ ìƒì„±\n",
    "\n",
    "sample_images_img = np.reshape(sample_images.data.cpu().numpy()\n",
    "                               [0],(28, 28))\n",
    "plt.imshow(sample_images_img, cmap = 'gray')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
